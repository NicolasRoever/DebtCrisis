{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get GPT Sentiment Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gpt_35 = pd.read_csv(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt Crisis/debt_crisis/src/debt_crisis/data/GPT_Output_Data/sentiment_data_portugal_output_v003.csv\",\n",
    ")\n",
    "data_gpt_4 = pd.read_csv(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt Crisis/debt_crisis/src/debt_crisis/data/GPT_Output_Data/sentiment_data_portugal_output_v005.csv\",\n",
    ")\n",
    "clean_transcripts = pd.read_pickle(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt Crisis/debt_crisis/bld/data/df_transcripts_raw.pkl\",\n",
    ")\n",
    "training_data = pd.read_pickle(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt Crisis/debt_crisis/bld/data/gpt_sentiment_data/df_gpt_sentiment_training_dataset_cleaned.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 35 Dataset\n",
    "# Add Transcript_ID\n",
    "full_data_35 = data_gpt_35.merge(\n",
    "    training_data,\n",
    "    how=\"left\",\n",
    "    left_on=\"Snippet_ID\",\n",
    "    right_on=\"Snippet_ID\",\n",
    "    validate=\"one_to_one\",\n",
    ")\n",
    "\n",
    "# Check\n",
    "are_columns_equal = full_data_35[\"Snippet\"] == full_data_35[\"Excerpt\"]\n",
    "# Check if all values in the two columns are equal\n",
    "all_equal = are_columns_equal.all()\n",
    "\n",
    "# To see the result\n",
    "print(all_equal)\n",
    "full_data_35 = full_data_35.merge(\n",
    "    clean_transcripts,\n",
    "    how=\"left\",\n",
    "    left_on=\"Transcript_ID\",\n",
    "    right_on=\"Transcript_ID\",\n",
    "    validate=\"many_to_one\",\n",
    ")\n",
    "# Count the number of rows before dropping NaN values\n",
    "before_drop = len(full_data_35)\n",
    "\n",
    "# Drop rows with NaN values in the \"Prediction\" column\n",
    "full_data_35 = full_data_35.dropna(subset=[\"Prediction\"])\n",
    "\n",
    "# Count the number of rows after dropping NaN values\n",
    "after_drop = len(full_data_35)\n",
    "\n",
    "# Calculate and print the number of observations dropped\n",
    "observations_dropped = before_drop - after_drop\n",
    "print(f\"Number of observations dropped: {observations_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make 4 Dataset\n",
    "# Add Transcript_ID\n",
    "full_data_4 = data_gpt_4.merge(\n",
    "    training_data,\n",
    "    how=\"left\",\n",
    "    left_on=\"Snippet_ID\",\n",
    "    right_on=\"Snippet_ID\",\n",
    "    validate=\"one_to_one\",\n",
    ")\n",
    "\n",
    "# Check\n",
    "are_columns_equal = full_data_4[\"Snippet\"] == full_data_4[\"Excerpt\"]\n",
    "# Check if all values in the two columns are equal\n",
    "all_equal = are_columns_equal.all()\n",
    "\n",
    "# To see the result\n",
    "print(all_equal)\n",
    "full_data_4 = full_data_4.merge(\n",
    "    clean_transcripts,\n",
    "    how=\"left\",\n",
    "    left_on=\"Transcript_ID\",\n",
    "    right_on=\"Transcript_ID\",\n",
    "    validate=\"many_to_one\",\n",
    ")\n",
    "# Count the number of rows before dropping NaN values\n",
    "before_drop = len(full_data_4)\n",
    "\n",
    "# Drop rows with NaN values in the \"Prediction\" column\n",
    "full_data_4 = full_data_4.dropna(subset=[\"Prediction\"])\n",
    "\n",
    "# Count the number of rows after dropping NaN values\n",
    "after_drop = len(full_data_4)\n",
    "\n",
    "# Calculate and print the number of observations dropped\n",
    "observations_dropped = before_drop - after_drop\n",
    "print(f\"Number of observations dropped: {observations_dropped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gpt_sentiment_index(\n",
    "    preprocessed_data,\n",
    "    countries_under_study,\n",
    "    day_window=90,\n",
    "):\n",
    "    \"\"\"This function calculates the sentiment index taking as input the preprocessed\n",
    "    data generated by earlier functions in this script.\n",
    "\n",
    "    Args:\n",
    "        preprocessed_data (pd.DataFrame): Dataframe with the data from gpt\n",
    "        countries_under_study (list): List of countries to consider\n",
    "        day_window (int): Number of days to consider for the sentiment index\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with the sentiment index\n",
    "        columns: Date (pd.DateTime): Date of sentiment index\n",
    "                Sentiment_Index_country (int): Sentiment index for the country (there is one of such columns for every country under study.)\n",
    "    \"\"\"\n",
    "    # Ensure 'Date' column in preprocessed_data is of datetime type\n",
    "    preprocessed_data[\"Date\"] = pd.to_datetime(preprocessed_data[\"Date\"])\n",
    "\n",
    "    # Create date range from January 2003 to January 2023\n",
    "    date_range = pd.date_range(start=\"1/1/2003\", end=\"1/1/2023\")\n",
    "\n",
    "    # Initialize a DataFrame with 'Date' column\n",
    "    result_df = pd.DataFrame(date_range, columns=[\"Date\"])\n",
    "\n",
    "    # Set 'Date' as index for efficient lookup\n",
    "    result_df = result_df.set_index(\"Date\")\n",
    "    preprocessed_data = preprocessed_data.set_index(\"Date\")\n",
    "\n",
    "    # Iterate over each date\n",
    "    for date in date_range:\n",
    "        # Iterate over each country\n",
    "        for country in countries_under_study:\n",
    "            # Calculate the sum of the Sentiment_Index_McDonald_{country} column over the prior day_window days\n",
    "            end_date = date\n",
    "            start_date = end_date - pd.Timedelta(\n",
    "                days=day_window,\n",
    "            )  # start date is day_window days before the end date\n",
    "\n",
    "            # Extract the data for the window\n",
    "            # Filter for observations within the date range using boolean indexing\n",
    "            mask = (preprocessed_data.index >= start_date) & (\n",
    "                preprocessed_data.index <= end_date\n",
    "            )\n",
    "            window_data = preprocessed_data.loc[mask, \"Prediction\"]\n",
    "\n",
    "            # Calculate the sentiment index\n",
    "            sentiment_index = (\n",
    "                window_data.sum() / len(window_data) if len(window_data) > 0 else np.nan\n",
    "            )\n",
    "\n",
    "            # Add the sentiment index to the result DataFrame\n",
    "            result_df.loc[date, f\"Sentiment_GPT_{country}\"] = sentiment_index\n",
    "\n",
    "    return result_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_sentiment_index_35 = calculate_gpt_sentiment_index(\n",
    "    preprocessed_data=full_data_35,\n",
    "    countries_under_study=[\"portugal\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_sentiment_index_4 = calculate_gpt_sentiment_index(\n",
    "    preprocessed_data=full_data_4,\n",
    "    countries_under_study=[\"portugal\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.plot(\n",
    "    gpt_sentiment_index_35[\"Date\"],\n",
    "    gpt_sentiment_index_35[\"Sentiment_GPT_portugal\"],\n",
    "    label=\"Sentiment GPT Portugal\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "# Adding the time series from full_data_gpt_4\n",
    "plt.plot(\n",
    "    gpt_sentiment_index_4[\"Date\"],\n",
    "    gpt_sentiment_index_4[\"Sentiment_GPT_portugal\"],\n",
    "    label=\"Sentiment GPT Index 4\",\n",
    "    marker=\"x\",\n",
    ")\n",
    "plt.title(\"Sentiment GPT Portugal Over Time\")  # Title of the plot\n",
    "plt.xlabel(\"Date\")  # X-axis label\n",
    "plt.ylabel(\"Sentiment GPT Score\")  # Y-axis label\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Correlation\n",
    "# Step 1: Ensure 'Date' is the index and is of datetime type\n",
    "gpt_sentiment_index_35[\"Date\"] = pd.to_datetime(gpt_sentiment_index_35[\"Date\"])\n",
    "gpt_sentiment_index_4[\"Date\"] = pd.to_datetime(gpt_sentiment_index_4[\"Date\"])\n",
    "gpt_sentiment_index_35 = gpt_sentiment_index_35.set_index(\"Date\")\n",
    "gpt_sentiment_index_4 = gpt_sentiment_index_4.set_index(\"Date\")\n",
    "\n",
    "# Step 2: Align both series on 'Date'\n",
    "aligned_data = gpt_sentiment_index_35[[\"Sentiment_GPT_portugal\"]].merge(\n",
    "    gpt_sentiment_index_4[[\"Sentiment_GPT_portugal\"]],\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    suffixes=(\"_35\", \"_4\"),\n",
    ")\n",
    "\n",
    "# Step 3: Calculate the correlation\n",
    "correlation = aligned_data.corr().iloc[0, 1]\n",
    "correlation\n",
    "gpt_sentiment_index_35 = gpt_sentiment_index_35.reset_index()\n",
    "gpt_sentiment_index_4 = gpt_sentiment_index_4.reset_index(\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get normal senitment index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcdonald_sentiment_index = pd.read_pickle(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt Crisis/debt_crisis/bld/data/mcdonald_sentiment_index_negative_and_positive_20_.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.plot(\n",
    "    mcdonald_sentiment_index[\"Date\"],\n",
    "    mcdonald_sentiment_index[\"Sentiment_Index_McDonald_portugal\"],\n",
    "    label=\"Sentiment McDonald Portugal\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "plt.title(\"Sentiment GPT Portugal Over Time\")  # Title of the plot\n",
    "plt.xlabel(\"Date\")  # X-axis label\n",
    "plt.ylabel(\"Sentiment GPT Score\")  # Y-axis label\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEt bond yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yield_data = pd.read_pickle(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt Crisis/debt_crisis/bld/data/financial_data/Quarterly Macroeconomic Variables_cleaned.pkl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter for Portugal\n",
    "portugal_data = yield_data[yield_data[\"Country\"] == \"portugal\"]\n",
    "\n",
    "# Step 2: Convert 'Date' column to datetime format\n",
    "portugal_data[\"Date\"] = pd.to_datetime(portugal_data[\"Date\"])\n",
    "\n",
    "\n",
    "# Step 4: Plotting\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.plot(\n",
    "    portugal_data[\"Date\"],\n",
    "    portugal_data[\"10y_Maturity_Bond_Yield\"],\n",
    "    label=\"10y Maturity Bond Yield for Portugal\",\n",
    "    marker=\"o\",\n",
    ")\n",
    "plt.title(\"10y Maturity Bond Yield for Portugal Over Time\")  # Title of the plot\n",
    "plt.xlabel(\"Date\")  # X-axis label\n",
    "plt.ylabel(\"10y Maturity Bond Yield (%)\")  # Y-axis label\n",
    "plt.legend()  # Show legend\n",
    "plt.grid(True)  # Show grid\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to not cut off labels\n",
    "plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all data in one plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_dates = portugal_data[\"Date\"]\n",
    "gpt_quarter_data = gpt_sentiment_index_4[\n",
    "    gpt_sentiment_index_4[\"Date\"].isin(quarter_dates)\n",
    "]\n",
    "mcdonald_quarter_data = mcdonald_sentiment_index[\n",
    "    mcdonald_sentiment_index[\"Date\"].isin(quarter_dates)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Plotting\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot sentiment from full_data\n",
    "ax1.plot(\n",
    "    gpt_quarter_data[\"Date\"],\n",
    "    gpt_quarter_data[\"Sentiment_GPT_portugal\"],\n",
    "    label=\"Sentiment GPT Portugal\",\n",
    "    alpha=0.9,\n",
    ")\n",
    "ax1.set_xlabel(\"Date\")\n",
    "ax1.set_ylabel(\"Sentiment from GPT\", color=\"blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "\n",
    "# Create a second y-axis for bond yield\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    portugal_data[\"Date\"],\n",
    "    portugal_data[\"10y_Maturity_Bond_Yield\"],\n",
    "    label=\"Bond Yield\",\n",
    "    color=\"red\",\n",
    ")\n",
    "ax2.set_ylabel(\"Bond Yield\", color=\"red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "\n",
    "# Create a third y-axis for sentiment in df\n",
    "ax3 = ax1.twinx()\n",
    "# Offset the right spine of ax3. The ticks and label have already been\n",
    "# colored in ax2, so only the spine needs to be colored.\n",
    "ax3.spines[\"right\"].set_position((\"outward\", 60))  # Offset the third axis\n",
    "ax3.plot(\n",
    "    mcdonald_quarter_data[\"Date\"],\n",
    "    mcdonald_quarter_data[\"Sentiment_Index_McDonald_portugal\"],\n",
    "    label=\"Sentiment McDonald Portugal\",\n",
    "    color=\"green\",\n",
    "    alpha=0.9,\n",
    ")\n",
    "ax3.set_ylabel(\"Sentiment from Loughran and McDonald\", color=\"green\")\n",
    "ax3.tick_params(axis=\"y\", labelcolor=\"green\")\n",
    "\n",
    "# Optional: Add a legend or grid\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"lower left\")\n",
    "ax3.legend(loc=\"upper right\")\n",
    "ax1.grid(True)\n",
    "\n",
    "plt.title(\"Sentiment and Bond Yield Over Time for Portugal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Correlation Matix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter for quarter dates\n",
    "quarter_dates = portugal_data[\"Date\"]\n",
    "gpt_quarter_data = gpt_sentiment_index_4[\n",
    "    gpt_sentiment_index_4[\"Date\"].isin(quarter_dates)\n",
    "]\n",
    "mcdonald_quarter_data = mcdonald_sentiment_index[\n",
    "    mcdonald_sentiment_index[\"Date\"].isin(quarter_dates)\n",
    "]\n",
    "\n",
    "# Step 2: Merge DataFrames\n",
    "# Ensure that the Date columns are of the same data type to avoid merge issues\n",
    "merged_data = pd.merge(portugal_data, gpt_quarter_data, on=\"Date\", how=\"inner\")\n",
    "merged_data = pd.merge(merged_data, mcdonald_quarter_data, on=\"Date\", how=\"inner\")\n",
    "\n",
    "# Rename columns for clarity if needed\n",
    "merged_data = merged_data.rename(\n",
    "    columns={\n",
    "        \"10y_Maturity_Bond_Yield\": \"Bond_Yield\",\n",
    "        \"Sentiment_GPT_portugal\": \"GPT_Index\",\n",
    "        \"Sentiment_Index_McDonald_portugal\": \"McDonald_Index\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Step 3: Calculate Correlation Matrix\n",
    "correlation_matrix = merged_data[[\"Bond_Yield\", \"GPT_Index\", \"McDonald_Index\"]].corr()\n",
    "\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix between Bond Yield, GPT Index, and McDonald Index\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
