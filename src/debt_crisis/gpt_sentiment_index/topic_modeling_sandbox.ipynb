{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling with BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from bertopic import BERTopic\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from umap import UMAP\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Set up loggings\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Topic with Highest Probability\n",
    "\n",
    "In this script, I cluster the rationales based on the topic with the highest probability, thus I try to minimize -1 classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt_Crisis/debt_crisis/bld/data/topic_model_intermediate/text_snippets_lemmatized_v003.csv\",\n",
    ") as file:\n",
    "    reader = csv.reader(file)\n",
    "    text_snippets = next(reader)\n",
    "embeddings = np.load(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt_Crisis/debt_crisis/bld/data/topic_model_intermediate/embeddings.npy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_BERT_model_and_return_topic_probabilities(\n",
    "    text_snippets: list[str],\n",
    "    embeddings: np.ndarray,\n",
    "    min_topic_size: int = 50,\n",
    "    n_neighbors: int = 5,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Fit a BERTopic model on the text snippets and embeddings and return a dataframe with text snippets, topics and probabilities.\"\"\"\n",
    "    # Create vectorizer and UMAP models with the specified hyperparameters\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, random_state=42)\n",
    "\n",
    "    # Initialize BERTopic with the vectorizer and embeddings\n",
    "    topic_model = BERTopic(\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        calculate_probabilities=True,\n",
    "        min_topic_size=min_topic_size,\n",
    "        umap_model=umap_model,\n",
    "    )\n",
    "\n",
    "    # Fit the model on text snippets and embeddings\n",
    "    topics, probabilities = topic_model.fit_transform(text_snippets, embeddings)\n",
    "\n",
    "    # Convert the probabilities array to a DataFrame\n",
    "    probabilities_df = pd.DataFrame(\n",
    "        probabilities,\n",
    "        columns=[f\"Topic_Probability_{i}\" for i in range(probabilities.shape[1])],\n",
    "    )\n",
    "\n",
    "    # Create a DataFrame with text snippets and topic probabilities\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Text Snippet\": text_snippets,\n",
    "            \"Topic\": topics,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    df = pd.concat([df, probabilities_df], axis=1)\n",
    "\n",
    "    return df, topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_assign_topics(df: pd.DataFrame, threshold: float):\n",
    "    \"\"\"Self-assigns topics based on topic probabilities. Assigns the topic with the highest probability\n",
    "    if the difference between the highest and second highest probability exceeds the threshold.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing text snippets and topic probabilities.\n",
    "    threshold (float): The minimum difference required between the highest and second highest probabilities to assign a topic.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Updated DataFrame with a new column for self-assigned topics.\n",
    "    \"\"\"\n",
    "    # Extract only the probability columns\n",
    "    probability_columns = [\n",
    "        col for col in df.columns if col.startswith(\"Topic_Probability\")\n",
    "    ]\n",
    "\n",
    "    # Create a new column for self-assigned topics\n",
    "    df[\"Self_Assigned_Topic\"] = -1  # Default to -1 for unassigned topics\n",
    "\n",
    "    # Iterate over each row to determine self-assigned topics\n",
    "    for idx, row in df.iterrows():\n",
    "        # Get the probabilities for the current row\n",
    "        probabilities = row[probability_columns].values\n",
    "\n",
    "        # Find the indices of the highest and second highest probabilities\n",
    "        highest_index = np.argmax(probabilities)\n",
    "        second_highest_index = np.argsort(probabilities)[-2]\n",
    "\n",
    "        # Calculate the difference between the highest and second highest probabilities\n",
    "        difference = probabilities[highest_index] - probabilities[second_highest_index]\n",
    "\n",
    "        # Assign the topic if the difference exceeds the threshold\n",
    "        if difference > threshold:\n",
    "            df.at[idx, \"Self_Assigned_Topic\"] = highest_index\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select 10 random indices\n",
    "\n",
    "# # Select the random text snippets and corresponding embeddings\n",
    "# Display the DataFrame\n",
    "df, topic_model = fit_BERT_model_and_return_topic_probabilities(\n",
    "    text_snippets,\n",
    "    embeddings,\n",
    "    min_topic_size=10,\n",
    "    n_neighbors=5,\n",
    ")\n",
    "\n",
    "df = self_assign_topics(df, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "df[\"Self_Assigned_Topic\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.xlabel(\"Self_Assigned_Topic\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Self_Assigned_Topic Frequencies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_plot = topic_model.visualize_barchart(top_n_topics=10)\n",
    "frequency_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_freq().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "I want to tune hyperparameters to have a reasonable amount of topics and most snippets classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt_Crisis/debt_crisis/bld/data/topic_model_intermediate/text_snippets_lemmatized.csv\",\n",
    ") as file:\n",
    "    reader = csv.reader(file)\n",
    "    text_snippets = next(reader)\n",
    "embeddings = np.load(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt_Crisis/debt_crisis/bld/data/topic_model_intermediate/embeddings.npy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topic_model(\n",
    "    text_snippets: list[str],\n",
    "    embeddings: np.ndarray,\n",
    "    n_neighbors: int,\n",
    "    min_topic_size: int,\n",
    "    number_of_topics: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Evaluate BERTopic model and return the top 30 topics and their sizes.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    text_snippets : List[str]\n",
    "        A list of text snippets to be used for topic modeling. Each entry in the list\n",
    "        should be a string representing a text document.\n",
    "\n",
    "    embeddings : Any\n",
    "        Pre-computed embeddings for the text snippets. These are usually generated\n",
    "        using a sentence transformer or any compatible embedding model.\n",
    "        This parameter should be a NumPy array or a list of lists representing the embeddings.\n",
    "\n",
    "    n_neighbors : int\n",
    "        The number of neighboring points used in UMAP's local manifold approximation.\n",
    "        Lower values result in a more local approximation, while higher values produce\n",
    "        a more global view of the manifold.\n",
    "\n",
    "    min_topic_size : int\n",
    "        The minimum number of documents required to form a topic. Smaller values allow\n",
    "        the model to create smaller, more granular topics, whereas larger values ensure\n",
    "        topics are more significant in size.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing the top 30 topics generated by the BERTopic model,\n",
    "        with columns for 'Topic' and 'Frequency'. The 'Topic' column indicates the\n",
    "        topic label, and 'Frequency' shows how many documents are assigned to each topic.\n",
    "    \"\"\"\n",
    "    # Create vectorizer and UMAP models with the specified hyperparameters\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, random_state=42)\n",
    "\n",
    "    # Initialize BERTopic with the vectorizer and embeddings\n",
    "    topic_model = BERTopic(\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        umap_model=umap_model,\n",
    "        min_topic_size=min_topic_size,\n",
    "        calculate_probabilities=False,\n",
    "        nr_topics=number_of_topics,\n",
    "    )\n",
    "\n",
    "    # Fit the model on text snippets and embeddings\n",
    "    topics, _ = topic_model.fit_transform(text_snippets, embeddings)\n",
    "\n",
    "    # Get the top 30 topics and their frequencies\n",
    "    topic_freq = topic_model.get_topic_freq().head(30)\n",
    "\n",
    "    # Attempt to visualize the topics\n",
    "    try:\n",
    "        # Check if there are enough topics to visualize\n",
    "        if topic_freq.shape[0] > 0:\n",
    "            fig = topic_model.visualize_topics()\n",
    "            # Add a title with hyperparameters\n",
    "            plt.title(\n",
    "                f\"Topics Visualization (n_neighbors={n_neighbors}, min_topic_size={min_topic_size})\",\n",
    "            )\n",
    "\n",
    "            # Save the figure\n",
    "            filename = f\"topics_visualization_n{n_neighbors}_min{min_topic_size}.html\"\n",
    "            fig.write_html(filename)\n",
    "            print(f\"Saved topics visualization to {filename}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"No topics to visualize for n_neighbors={n_neighbors}, min_topic_size={min_topic_size}\",\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"Could not visualize topics for n_neighbors={n_neighbors}, min_topic_size={min_topic_size}: {e}\",\n",
    "        )\n",
    "\n",
    "    return topic_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 15\n",
    "min_topic_size = 10\n",
    "\n",
    "topic_freq = evaluate_topic_model(\n",
    "    text_snippets=text_snippets,\n",
    "    embeddings=embeddings,\n",
    "    n_neighbors=n_neighbors,\n",
    "    min_topic_size=min_topic_size,\n",
    ")\n",
    "\n",
    "# Log results\n",
    "num_topics_after_fitting = topic_freq.shape[0]\n",
    "topic_counts = topic_freq.set_index(\"Topic\").to_dict()[\"Count\"]\n",
    "print(f\"Found {num_topics_after_fitting} topics with the following counts:\")\n",
    "print(topic_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"n_neighbors\": [3, 5, 10],  # UMAP n_neighbors\n",
    "    \"min_topic_size\": [5, 10, 20],\n",
    "    \"num_topics\": [20, 40, 80],  # BERTopic min_topic_size\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Perform grid search with bootstrapping\n",
    "for n_neighbors in param_grid[\"n_neighbors\"]:\n",
    "    for min_topic_size in param_grid[\"min_topic_size\"]:\n",
    "        num_topics = 200\n",
    "\n",
    "        # Evaluate the topic model\n",
    "        topic_freq = evaluate_topic_model(\n",
    "            text_snippets=text_snippets,\n",
    "            embeddings=embeddings,\n",
    "            n_neighbors=n_neighbors,\n",
    "            min_topic_size=min_topic_size,\n",
    "            number_of_topics=num_topics,\n",
    "        )\n",
    "\n",
    "        # Log results\n",
    "        num_topics_after_fitting = topic_freq.shape[0]\n",
    "        topic_counts = topic_freq.set_index(\"Topic\").to_dict()[\"Count\"]\n",
    "\n",
    "        result = {\n",
    "            \"n_neighbors\": n_neighbors,\n",
    "            \"min_topic_size\": min_topic_size,\n",
    "            \"num_topics\": num_topics,\n",
    "            \"topic_counts\": topic_counts,\n",
    "        }\n",
    "        results.append(result)\n",
    "        print(\n",
    "            f\"n_neighbors: {n_neighbors}, min_topic_size: {min_topic_size}, num_topics: {num_topics_after_fitting}, topic_counts: {topic_counts}\",\n",
    "        )\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set_with_all_country_words(country_names_file):\n",
    "    # Flatten the DataFrame to a single list\n",
    "    country_words = country_names_file.values.flatten()\n",
    "\n",
    "    # Remove NaN values\n",
    "    country_words = [word for word in country_words if pd.notna(word)]\n",
    "\n",
    "    # Create a set of unique words\n",
    "    return set(country_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt_Crisis/debt_crisis/bld/data/GPT_Output_Data/sentiment_data_clean_full.pkl\",\n",
    ")\n",
    "country_names_file = pd.read_csv(\n",
    "    \"/Users/nicolasroever/Documents/Promotion/Debt_Crisis/debt_crisis/src/debt_crisis/data/country_names/country_names.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I delete all country words from the rationales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_set = create_set_with_all_country_words(country_names_file)\n",
    "country_words = {word for entry in countries_set for word in entry.split(\";\") if word}\n",
    "\n",
    "# Create a regex pattern that matches any of these words\n",
    "pattern = re.compile(\n",
    "    r\"\\b(\" + \"|\".join(map(re.escape, country_words)) + r\")\\b\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# Assuming df is your DataFrame and \"Rationale for Prediction\" is the column you want to modify\n",
    "data[\"Rationale_for_Prediction\"] = data[\"Rationale_for_Prediction\"].str.replace(\n",
    "    pattern,\n",
    "    \"\",\n",
    "    regex=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text: str) -> str:\n",
    "    \"\"\"Lemmatize the given text using spaCy.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): A string containing the text to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "    str: A string containing the lemmatized version of the input text,\n",
    "         with only alphabetic tokens included.\n",
    "    \"\"\"\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "    # Extract the lemma for each token and filter out non-alphabetic tokens\n",
    "    return \" \".join(token.lemma_ for token in doc if token.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data[\"Rationale_for_Prediction\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_subset = data.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a progress bar to the pandas apply method\n",
    "tqdm.pandas(desc=\"Lemmatizing Texts\")\n",
    "\n",
    "# Apply lemmatization to the \"Rationale_for_Prediction\" column\n",
    "data_subset[\"Rationale_for_Prediction_Lemmatized\"] = data_subset[\n",
    "    \"Rationale_for_Prediction\"\n",
    "].progress_apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_snippets = data_subset[\"Rationale_for_Prediction_Lemmatized\"].to_list()\n",
    "timestamps = pd.to_datetime(data_subset[\"Date\"])\n",
    "timestamps = timestamps.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run FinBert Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_text_snippets(\n",
    "    texts: list[str],\n",
    "    tokenizer: BertTokenizer = BertTokenizer.from_pretrained(\n",
    "        \"yiyanghkust/finbert-pretrain\",\n",
    "    ),\n",
    "    model: BertModel = BertModel.from_pretrained(\"yiyanghkust/finbert-pretrain\"),\n",
    "    max_length: int = 128,\n",
    "    batch_size: int = 32,\n",
    "    use_gpu: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for a list of texts using a pre-trained BERT-based model.\n",
    "\n",
    "    Parameters:\n",
    "    texts (List[str]): A list of text snippets to be encoded into embeddings.\n",
    "    tokenizer (BertTokenizer, optional): The tokenizer to process the input texts.\n",
    "    model (BertModel, optional): The BERT-based model to generate embeddings.\n",
    "    max_length (int, optional): The maximum length for tokenization. Defaults to 128.\n",
    "    batch_size (int, optional): The number of texts to process in a batch. Defaults to 32.\n",
    "    use_gpu (bool, optional): Whether to use GPU acceleration. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 2D numpy array where each row corresponds to the embedding of a text snippet.\n",
    "    \"\"\"\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    embeddings = []\n",
    "    total_texts = len(texts)\n",
    "\n",
    "    # Process texts in batches\n",
    "    for start_idx in range(0, total_texts, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_texts)\n",
    "        batch_texts = texts[start_idx:end_idx]\n",
    "\n",
    "        # Tokenize and encode the text data\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "        if use_gpu and torch.cuda.is_available():\n",
    "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "        # Generate embeddings without gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Mean pooling of token embeddings\n",
    "        mean_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.append(mean_embeddings)\n",
    "\n",
    "        # Print progress every 20 texts\n",
    "        if (end_idx) % 20 == 0:\n",
    "            progress = end_idx / total_texts * 100\n",
    "            print(f\"Progress: {progress:.2f}% ({end_idx}/{total_texts})\")\n",
    "\n",
    "    # Stack embeddings into a single numpy array\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings_for_text_snippets(\n",
    "    text_snippets,\n",
    "    batch_size=100,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer for BERTopic\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Initialize UMAP with adjusted parameters\n",
    "umap_model = UMAP(n_neighbors=10, random_state=42)\n",
    "\n",
    "# Initialize BERTopic with the vectorizer and FinBERT embeddings\n",
    "topic_model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    min_topic_size=15,\n",
    "    umap_model=umap_model,\n",
    "    calculate_probabilities=True,\n",
    ")\n",
    "\n",
    "logger.info(\"Starting topic modeling with BERTopic\")\n",
    "topics, probabilities = topic_model.fit_transform(text_snippets, embeddings)\n",
    "logger.info(\"Completed topic modeling with BERTopic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_freq().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_df = pd.DataFrame(\n",
    "    probabilities,\n",
    "    columns=[f\"Topic_{i}\" for i in range(probabilities.shape[1])],\n",
    ")\n",
    "probabilities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and print texts assigned to topic -1\n",
    "outlier_texts = [text for text, topic in zip(text_snippets, topics) if topic == -1]\n",
    "\n",
    "print(\"Text snippets assigned to topic -1:\")\n",
    "for idx, text in enumerate(outlier_texts[0:2]):\n",
    "    print(f\"{idx + 1}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(text_snippets, timestamps, nr_bins=20)\n",
    "topic_model.visualize_topics_over_time(topics_over_time, topics=[0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decompose Sentiment Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topic names\n",
    "topic_names = topic_model.get_topic_info()\n",
    "\n",
    "# Create a mapping of topic numbers to their names\n",
    "topic_name_dict = topic_names.set_index(\"Topic\")[\"Name\"].to_dict()\n",
    "\n",
    "# Map the topic numbers to their names\n",
    "data[\"topic\"] = [\n",
    "    topic_name_dict[topic] if topic != -1 else \"Outlier\" for topic in topics\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'topic' and sum the 'Prediction' column\n",
    "topic_sums = data.groupby(\"topic\")[\"Prediction\"].sum().reset_index()\n",
    "\n",
    "# Sort by the absolute value of the summed predictions and select the top 10\n",
    "top_10_topics = topic_sums.reindex(\n",
    "    topic_sums[\"Prediction\"].abs().sort_values(ascending=False).index,\n",
    ").head(10)\n",
    "\n",
    "# Plotting the column chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10_topics[\"topic\"], top_10_topics[\"Prediction\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Sum of Predictions\")\n",
    "plt.title(\"Top 10 Topics by Sum of Predictions\")\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotate labels and align them to the right\n",
    "plt.tight_layout()  # Adjust layout to make room for the rotated labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory where you want to save the model and tokenizer\n",
    "import pickle\n",
    "\n",
    "# Assume `topic_model` is your trained topic model\n",
    "with open(\"saved_bert_model_1000_lemma.pkl\", \"wb\") as f:\n",
    "    pickle.dump(topic_model, f)\n",
    "\n",
    "# Save embeddings as a .npy file\n",
    "np.save(\"embeddings_lemma.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_for_text_snippets(\n",
    "    texts: list[str],\n",
    "    tokenizer: BertTokenizer = BertTokenizer.from_pretrained(\n",
    "        \"yiyanghkust/finbert-pretrain\",\n",
    "    ),\n",
    "    model: BertModel = BertModel.from_pretrained(\"yiyanghkust/finbert-pretrain\"),\n",
    "    max_length: int = 128,\n",
    "    batch_size: int = 32,\n",
    "    use_gpu: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for a list of texts using a pre-trained BERT-based model.\n",
    "\n",
    "    Parameters:\n",
    "    texts (List[str]): A list of text snippets to be encoded into embeddings.\n",
    "    tokenizer (BertTokenizer, optional): The tokenizer to process the input texts.\n",
    "    model (BertModel, optional): The BERT-based model to generate embeddings.\n",
    "    max_length (int, optional): The maximum length for tokenization. Defaults to 128.\n",
    "    batch_size (int, optional): The number of texts to process in a batch. Defaults to 32.\n",
    "    use_gpu (bool, optional): Whether to use GPU acceleration. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 2D numpy array where each row corresponds to the embedding of a text snippet.\n",
    "    \"\"\"\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    embeddings = []\n",
    "    total_texts = len(texts)\n",
    "\n",
    "    # Process texts in batches\n",
    "    for start_idx in range(0, total_texts, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_texts)\n",
    "        batch_texts = texts[start_idx:end_idx]\n",
    "\n",
    "        # Tokenize and encode the text data\n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "        if use_gpu and torch.cuda.is_available():\n",
    "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "\n",
    "        # Generate embeddings without gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Mean pooling of token embeddings\n",
    "        mean_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.append(mean_embeddings)\n",
    "\n",
    "        # Print progress every 20 texts\n",
    "        if (end_idx) % 20 == 0 or end_idx == total_texts:\n",
    "            progress = end_idx / total_texts * 100\n",
    "            print(f\"Progress: {progress:.2f}% ({end_idx}/{total_texts})\")\n",
    "\n",
    "    # Stack embeddings into a single numpy array\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings_for_text_snippets_parallel(selected_text_snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add this to remove stopwords\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language=\"english\",\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    nr_topics=50,\n",
    ")\n",
    "\n",
    "topics, probabilities = model.fit_transform(selected_text_snippets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_freq().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_barchart(top_n_topics=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(topics[:5]):\n",
    "    print(f\"Document {i}: Topic {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = model.topics_over_time(\n",
    "    selected_text_snippets,\n",
    "    selected_timestamps,\n",
    "    nr_bins=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics_over_time(topics_over_time, topics=[0, 1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decompose Sentiment Score into Topic Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the topic names\n",
    "topic_names = model.get_topic_info()\n",
    "\n",
    "# Create a mapping of topic numbers to their names\n",
    "topic_name_dict = topic_names.set_index(\"Topic\")[\"Name\"].to_dict()\n",
    "\n",
    "# Map the topic numbers to their names\n",
    "full_data[\"topic\"] = [\n",
    "    topic_name_dict[topic] if topic != -1 else \"Outlier\" for topic in topics\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'topic' and sum the 'Prediction' column\n",
    "topic_sums = full_data.groupby(\"topic\")[\"Prediction\"].sum().reset_index()\n",
    "\n",
    "# Sort by the absolute value of the summed predictions and select the top 10\n",
    "top_10_topics = topic_sums.reindex(\n",
    "    topic_sums[\"Prediction\"].abs().sort_values(ascending=False).index,\n",
    ").head(10)\n",
    "\n",
    "# Plotting the column chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10_topics[\"topic\"], top_10_topics[\"Prediction\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Sum of Predictions\")\n",
    "plt.title(\"Top 10 Topics by Sum of Predictions\")\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotate labels and align them to the right\n",
    "plt.tight_layout()  # Adjust layout to make room for the rotated labels\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
