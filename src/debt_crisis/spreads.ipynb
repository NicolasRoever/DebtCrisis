{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62ec777",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FuncFormatter\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "## Import libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "## Formatting\n",
    "\n",
    "# Plots\n",
    "plt.rc(\"text\", usetex = True) # Use LaTeX to render text\n",
    "plt.rc(\"font\", family = \"serif\", size = 12) # Set font family and size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b715122",
   "metadata": {},
   "source": [
    "### Save and load data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f0434a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data frames to not re-load them in a new session\n",
    "# df.to_pickle(\"preprocessed_df.pkl\")\n",
    "# df_sentiment_index.to_pickle(\"sentiment_index.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0fbbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data frames from previous sessions\n",
    "df = pd.read_pickle(\"preprocessed_df.pkl\")\n",
    "df_sentiment_index = pd.read_pickle(\"sentiment_index.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ccb0a9",
   "metadata": {},
   "source": [
    "### Import earnings call transcripts, sentiment dictionary, dictionary of country names, and financial data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42a2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import earnings conference call transcripts \n",
    "\n",
    "# Define function to extract quarter and year information from transcripts\n",
    "def extract_quarter_year(transcript):\n",
    "    \n",
    "    # Regular expressions for quarter, half-year, and full-year\n",
    "    quarter_pattern = r\"Q\\d\\s*\"\n",
    "    half_year_pattern = r\"(?:Half|H1|H2)\\s*Year\\s*\"\n",
    "    full_year_pattern = r\"Full\\s*Year\\s*\"\n",
    "    year_pattern = r\"\\d{4}\"\n",
    "\n",
    "    # Search for all patterns and store their match objects and starting positions\n",
    "    matches = [(re.search(pattern, transcript), pattern) for pattern in (quarter_pattern, half_year_pattern, full_year_pattern)]\n",
    "    matches = [(match, pattern) for match, pattern in matches if match]\n",
    "\n",
    "    # Get the first occurring match\n",
    "    first_match = min(matches, key=lambda x: x[0].start()) if matches else (None, None)\n",
    "\n",
    "    # Extract quarter, half year, and full year information based on the first occurring match\n",
    "    if first_match[1] == quarter_pattern:\n",
    "        quarter = first_match[0].group(0).strip()\n",
    "        # Validate the quarter value\n",
    "        quarter_number = int(quarter[1:])\n",
    "        if quarter_number < 1 or quarter_number > 4:\n",
    "            quarter = None\n",
    "    elif first_match[1] == half_year_pattern:\n",
    "        quarter = \"HY\"\n",
    "    elif first_match[1] == full_year_pattern:\n",
    "        quarter = \"FY\"\n",
    "    else:\n",
    "        quarter = None\n",
    "\n",
    "    # Extract year information and store in year variable\n",
    "    year_match = re.search(year_pattern, transcript)\n",
    "    year = year_match.group(0) if year_match else None\n",
    "\n",
    "    return quarter, year\n",
    "\n",
    "# Define function to extract date of conference call and company name from file name and text from txt files\n",
    "def extract_data_from_file(file_path):\n",
    "    \n",
    "    # Extract date and company name from file name\n",
    "    date_list = os.path.basename(file_path).split(\"-\")[:3]\n",
    "    date_str = \"-\".join(date_list)\n",
    "    company = os.path.basename(file_path).split(\"-\")[3]\n",
    "    \n",
    "    # Transform date string into datetime object and remove time\n",
    "    date = pd.to_datetime(date_str, format = \"%Y-%b-%d\").date()\n",
    "\n",
    "    # Extract text from txt files\n",
    "    with open(file_path, \"r\", encoding = \"utf-8\") as f:\n",
    "        transcript = f.read()\n",
    "        quarter, year = extract_quarter_year(transcript)\n",
    "\n",
    "    return {\"date\": date, \"quarter\": quarter, \"year\": year, \"company\": company, \"transcript\": transcript}\n",
    "\n",
    "# Define function to get the date of the quarter end of the quarter discussed in the earnings conference call\n",
    "def get_quarter_end(row):\n",
    "    \n",
    "    # For full year, set quarter end to end of the discussed year\n",
    "    if row[\"quarter\"] == \"FY\":\n",
    "        return pd.Timestamp(year=int(row[\"year\"]), month=12, day=31)\n",
    "    \n",
    "    # For half year, set quarter end to the end of that half year of the date of the earnings conference call\n",
    "    elif row[\"quarter\"] == \"HY\":\n",
    "        if row[\"date\"].month <= 6:\n",
    "            return pd.Timestamp(year=row[\"date\"].year, month=6, day=30)\n",
    "        else:\n",
    "            return pd.Timestamp(year=row[\"date\"].year, month=12, day=31)\n",
    "    \n",
    "    # For quarterly calls, set date to end of respective quarter\n",
    "    else:\n",
    "        return pd.to_datetime(row[\"quarter_end\"]) + pd.offsets.QuarterEnd()\n",
    "\n",
    "# Define directory\n",
    "directory = r\"C:\\Users\\fabia\\Dropbox\\Dokumente\\4_UoC CGS-E\\2_Dissertation\\Projects\\5_Bond yield spreads\\Data\\Earnings conference call transcripts\\Eikon 2002 - 2022\"\n",
    "\n",
    "# Initialize list to store data\n",
    "data_list = []\n",
    "\n",
    "# Loop over files in directory and subdirectories and apply function to extract data from files\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            data_list.append(extract_data_from_file(file_path))\n",
    "\n",
    "# Create data frame from the list\n",
    "df = pd.DataFrame(data_list).sort_values(by=\"date\").reset_index(drop=True)\n",
    "\n",
    "# Combine \"year\"and \"quarter\" columns into a new \"quarter_end\" column\n",
    "df[\"quarter_end\"] = df[\"year\"].astype(str) + df[\"quarter\"]\n",
    "\n",
    "# Apply function and set quarter end date as index, and drop quarter_end column\n",
    "df.index = df.apply(get_quarter_end, axis=1)\n",
    "df.index.name = \"quarter_end\"\n",
    "df = df.drop(\"quarter_end\", axis = 1)\n",
    "\n",
    "# Filter out entries for which there is no \"quarter\" value\n",
    "df = df[df['quarter'].notnull()]\n",
    "\n",
    "## Filter out full year reports, when there are half year reports for the same company in the same year\n",
    "\n",
    "# Select half year calls\n",
    "hy_calls = df[df[\"quarter\"] == \"HY\"]\n",
    "\n",
    "# Create a set of tuples containing the company and the year of half year calls\n",
    "hy_company_year = set(hy_calls[[\"company\", \"year\"]].apply(tuple, axis=1))\n",
    "\n",
    "# Define function that returns rows that have both full year and half year calls for the same company and year\n",
    "def to_remove(row):\n",
    "    return (row[\"quarter\"] == \"FY\") and ((row[\"company\"], row[\"year\"]) in hy_company_year)\n",
    "\n",
    "# Apply the function to each row and get a boolean series indicating whether to remove the row or not\n",
    "fy_rows_to_remove = df.apply(to_remove, axis=1)\n",
    "\n",
    "# Remove full year entries for which there are also half year entries \n",
    "df = df[~fy_rows_to_remove]\n",
    "\n",
    "# Add column alternative to quarter_end that takes the quarter end of the quarter previous to the date of the conference instead of from transcript - might deal better with companies having different financial year ends\n",
    "df[\"date_quarter\"] = df[\"date\"] - pd.offsets.QuarterEnd(n=1)\n",
    "df.insert(1, \"date_quarter\", df.pop(\"date_quarter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "083d906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dictionary of country names\n",
    "\n",
    "# Load country file and convert to string\n",
    "country_names = pd.read_excel(r\"data\\country_names\\country_names.xlsx\")\n",
    "country_names = country_names.astype(str)\n",
    "\n",
    "# Create list with country names\n",
    "countries_list = country_names.iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ef3c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Loughran & McDonald sentiment dictionary\n",
    "\n",
    "# Import word and positive/negative tone value from dictionary\n",
    "sentiment_dict = pd.read_csv(r\"data\\sentiment_dictionary\\Loughran-McDonald_MasterDictionary_1993-2021.csv\", usecols = [\"Word\", \"Negative\", \"Positive\"])\n",
    "\n",
    "# Convert words to lowercase\n",
    "sentiment_dict[\"Word\"] = sentiment_dict[\"Word\"].str.lower()\n",
    "\n",
    "# Create python dictionary for faster lookups later\n",
    "sentiment_dict = {row[\"Word\"]: row[\"Positive\"] - row[\"Negative\"] for _, row in sentiment_dict.iterrows()}\n",
    "\n",
    "# Transform negative, positive, and neutral sentiment scores into -1, +1, and 0, as current values represent year added\n",
    "sentiment_dict = {key: (1 if value > 0 else -1 if value < 0 else 0) for key, value in sentiment_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee28f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import financial data\n",
    "\n",
    "# Define funtion to read quarterly data\n",
    "def read_quarterly(filename):\n",
    "    df = pd.read_csv(os.path.join(r\"data\\financial_data\", filename))\n",
    "    \n",
    "    # Transform to datetime\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"]) + pd.offsets.QuarterEnd() \n",
    "    \n",
    "    # Set date as index\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "    \n",
    "    # Sort by ascending date\n",
    "    df = df.sort_values(by = \"Date\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define function to read monthly data and transform into quarterly\n",
    "def read_monthly(filename):\n",
    "    df = pd.read_csv(os.path.join(r\"data\\financial_data\", filename))\n",
    "\n",
    "    # Transform to datetime\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%Y%b\")\n",
    "    \n",
    "    # Set date as index\n",
    "    df.set_index(\"Date\", inplace = True)\n",
    "    \n",
    "    # Average monthly values to construct quarterly values\n",
    "    df = df.resample(\"Q\").mean(numeric_only=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# GDP\n",
    "gdp = read_quarterly(\"gdp_EU.csv\")\n",
    "\n",
    "# Year-on-year change in gdp\n",
    "gdp_yoy = gdp.pct_change(periods = 4)\n",
    "\n",
    "# Debt-to-gdp\n",
    "debt_to_gdp = read_quarterly(\"debt-to-gdp_EU.csv\")\n",
    "\n",
    "# Year-on-year change in debt-to-gdp\n",
    "debt_to_gdp_yoy = debt_to_gdp.pct_change(periods = 4)\n",
    "\n",
    "# Current account\n",
    "current_account = read_quarterly(\"current account_EU.csv\")\n",
    "\n",
    "# Harmonized index of consumer prices\n",
    "hicp = read_monthly(\"hicp_EU_monthly.csv\")\n",
    "\n",
    "# Officical reserves\n",
    "official_reserves = read_monthly(\"official reserves_EU_monthly.csv\")\n",
    "\n",
    "# Construct current account / gdp\n",
    "current_account_over_gdp = current_account.div(gdp)\n",
    "\n",
    "# Construct official reserves / gdp\n",
    "official_reserves_over_gdp = official_reserves.div(gdp)\n",
    "\n",
    "# EURO STOXX 50 & VSTOXX\n",
    "stoxx = pd.read_excel(r\"data\\financial_data\\stoxx50_vstoxx.xlsx\")\n",
    "stoxx[\"Date\"] = pd.to_datetime(stoxx[\"Date\"]) + pd.offsets.QuarterEnd()\n",
    "stoxx.set_index(\"Date\", inplace = True)\n",
    "\n",
    "# Bond yields - missing estonia, latvia, and luxembourg\n",
    "yields = pd.read_excel(r\"data\\financial_data\\eu_yields_10y.xlsx\")\n",
    "yields[\"date\"] = pd.to_datetime(yields[\"date\"]) + pd.offsets.QuarterEnd()\n",
    "yields.set_index(\"date\", inplace = True)\n",
    "\n",
    "# Bond yield spreads - subtracting German bond yield\n",
    "yield_spreads = pd.DataFrame()\n",
    "for col in yields.columns:\n",
    "    if col == \"germany\":\n",
    "        continue\n",
    "    yield_spread = yields[col].sub(yields[\"germany\"])\n",
    "    yield_spreads[col] = yield_spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fb03eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import country long-term credit ratings\n",
    "\n",
    "# Initialize an empty data frame to store the data\n",
    "ratings = pd.DataFrame()\n",
    "\n",
    "# Loop over each file in the directory\n",
    "for filename in os.listdir(r\"data\\financial_data\\ratings\"):\n",
    "    if filename.endswith(\".xlsx\") and filename.startswith(\"Ratings_\"):\n",
    "        # Read the excel file into a pandas data frame\n",
    "        filepath = os.path.join(r\"data\\financial_data\\ratings\", filename)\n",
    "        temp_ratings = pd.read_excel(filepath, usecols=[\"Date\", \"Issuer Rating\", \"Rating Source\"])\n",
    "        \n",
    "        # Filter out rows with NULL values in the 'Date' column or 'WD' / \"RD\" in the 'Issuer Rating' column\n",
    "        temp_ratings = temp_ratings[pd.notnull(temp_ratings[\"Date\"]) & (~temp_ratings[\"Issuer Rating\"].isin([\"WD\", \"RD\", \"NR\"]))]\n",
    "        \n",
    "        # Convert the 'Date' column to datetime format and set as index\n",
    "        temp_ratings[\"Date\"] = pd.to_datetime(temp_ratings[\"Date\"], format = \"%d.%m.%Y\")\n",
    "        temp_ratings.set_index('Date', inplace=True)\n",
    "        \n",
    "        # Extract the word after \"Ratings_\" in the file name and rename the columns\n",
    "        country = filename.split(\"_\", 1)[1].split(\".\")[0]\n",
    "        temp_ratings.rename(columns={\"Issuer Rating\": \"rating\", \"Rating Source\": \"source\"}, inplace=True)\n",
    "        temp_ratings[\"country\"] = country\n",
    "        \n",
    "        # Concatenate the temporary data frame to the main data frame\n",
    "        ratings = pd.concat([ratings, temp_ratings], ignore_index = False)\n",
    "\n",
    "# Sort the index of the final data frame in ascending order\n",
    "ratings.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1a1da3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert ratings to numerical scale, combine Moody's and Fitch ratings and aggregate to quarterly data\n",
    "\n",
    "# Import conversion scale\n",
    "conversion_ratings = pd.read_excel(r\"data\\financial_data\\ratings\\conversion_ratings.xlsx\")\n",
    "\n",
    "# Convert ratings to numerical scale\n",
    "merged_df = ratings.merge(conversion_ratings, on = [\"source\", \"rating\"], how = \"left\")\n",
    "merged_df = merged_df.drop(columns = [\"rating\"]).rename(columns = {\"score\": \"rating\"})\n",
    "merged_df.index = ratings.index\n",
    "ratings.update(merged_df[\"rating\"])\n",
    "\n",
    "# Transform into quarterly data\n",
    "ratings.index = ratings.index.to_series().apply(lambda x: x + pd.offsets.QuarterEnd())\n",
    "\n",
    "# Combine Moody's and Fitch ratings\n",
    "ratings = ratings.reset_index()\n",
    "ratings = ratings.groupby([\"Date\", \"country\"])[\"rating\"].mean().reset_index()\n",
    "ratings = ratings.set_index(\"Date\")\n",
    "\n",
    "# Make format structurally similar to other data\n",
    "ratings = pd.pivot_table(ratings, values = \"rating\", index = ratings.index, columns = \"country\")\n",
    "\n",
    "# Fill in rating for quarters, where rating has not been changed\n",
    "ratings = ratings.fillna(method = \"ffill\", axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54605d84",
   "metadata": {},
   "source": [
    "### Transcript preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ea1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English model of spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define function for text preprocessing of earnings conference call transcripts\n",
    "def preprocess(text):\n",
    "    \n",
    "    # Remove content before \"Presentation\"\n",
    "    text = re.sub(r\".*Presentation\\n-*\\n\", \"\", text, flags = re.DOTALL)\n",
    "    \n",
    "    # Remove content after \"PRELIMINARY TRANSCRIPT:\"\n",
    "    text = re.sub(r\"PRELIMINARY TRANSCRIPT:.*\", \"\", text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove last word\n",
    "    text = re.sub(r'\\b\\w+\\b(?=[^\\w]*$)', '', text)\n",
    "    \n",
    "    # Remove lines with description of speakers in Q&A (identification based on number in square brackets)\n",
    "    text = re.sub(r\"^.*\\[\\d+\\].*$\\n?\", \"\", text, flags = re.MULTILINE)\n",
    "    \n",
    "    # Remove all \"-\" and \"=\" that appear more than two times after another\n",
    "    text = re.sub(r\"[=-]{3,}\", \"\", text)\n",
    "    \n",
    "    # Remove dates\n",
    "    text = re.sub(r\"\\d{1,2}(\\s*(th|st|nd|rd))?(\\s*(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec))?(\\s*\\d{2,4})?\", \"\", text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    # Convert to lowercase and remove non-alphabetic characters\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower())\n",
    "\n",
    "    # Tokenize text and remove stop words using spaCy\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ed8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to run preprocessing on multiple CPU cores to improve speed\n",
    "def apply_preprocess_parallel(df, func, column_name, num_threads=4):\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        \n",
    "        # Apply preprocessing function in parallel using 4 threads\n",
    "        results = list(executor.map(func, df[column_name]))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765b3d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing of earnings conference call transcripts\n",
    "\n",
    "# Only run function if it has not been run before\n",
    "if \"transcript_preprocessed\" not in df.columns:\n",
    "    \n",
    "    # Apply preprocessing function to all transcripts using 4 threads\n",
    "    df[\"transcript_preprocessed\"] = apply_preprocess_parallel(df, preprocess, \"transcript\", num_threads=4)\n",
    "    \n",
    "    # Delete unprocessed transcripts\n",
    "    df.drop(\"transcript\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ed66cf",
   "metadata": {},
   "source": [
    "### Sentiment analysis of transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a43c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get sentiment score of a word from sentiment dictionary\n",
    "def get_sentiment_score(word):\n",
    "    \n",
    "    # Retrieve sentiment score of a word; if word is not in the dicitonary, return 0\n",
    "    return sentiment_dict.get(word, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64e7ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get aggregate sentiment score (sentiment words +/- 10 words) associated with country in transcript\n",
    "def get_country_sentiment(transcript, country_word):\n",
    "    \n",
    "    # Initialize empty list to store sentiment score\n",
    "    sentiment_scores = []\n",
    "    \n",
    "    # Loop over each token in transcript using \"enumerate\" to keep track of position of token in transcript\n",
    "    for i, token in enumerate(transcript):\n",
    "        \n",
    "        # Proceed if token matches country word\n",
    "        if token == country_word:\n",
    "            \n",
    "            # Define context window in which sentiment words around country are counted\n",
    "            start = max(0, i - 10)\n",
    "            end = min(len(transcript), i + 11)\n",
    "            \n",
    "            # Create list that contains all tokens - except country word - in context window\n",
    "            context = [transcript[j] for j in range(start, end) if j != i]\n",
    "            \n",
    "            # Assign sentiment score to country word based on sentiment words in context window\n",
    "            sentiment_scores.append(sum([get_sentiment_score(token) for token in context]))\n",
    "    \n",
    "    # Return sum of sentiment scores\n",
    "    return sum(sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to count country words per transcript\n",
    "def count_country_words(transcript, country_names):\n",
    "    \n",
    "    # Count 1 if a word in transcript is in the country names dictionary\n",
    "    return sum(1 for word in transcript if any(word in country_words for country_words in country_names.values))\n",
    "\n",
    "# Apply function and create column in transcript data frame with total number of country ocurrences in transcript\n",
    "df[\"country_count\"] = df[\"transcript_preprocessed\"].apply(lambda x: count_country_words(x, country_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7bbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get total sentiment score for each country per transcript \n",
    "def get_total_sentiment_score(transcript):\n",
    "    \n",
    "    # Initialize empty dictionary\n",
    "    total_sentiment_score = {}\n",
    "    \n",
    "    # Loop over country names\n",
    "    for country_words in country_names.values:\n",
    "        \n",
    "        # Assign country and its equivalents its total number of ocurrences per transcript\n",
    "        total_sentiment_score[country_words[0]] = sum(get_country_sentiment(transcript, word) for word in country_words)\n",
    "    \n",
    "    return total_sentiment_score\n",
    "\n",
    "# Calculate total sentiment scores for each country in a separate data frame\n",
    "sentiment_scores_df = df['transcript_preprocessed'].apply(lambda x: pd.Series(get_total_sentiment_score(x)))\n",
    "\n",
    "# Concatenate the separate data frame with sentiment scores to the transcripts data frame\n",
    "df = pd.concat([df, sentiment_scores_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b952701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting of scores: divide country sentiment score by total number of discussed countries\n",
    "for country in countries_list:\n",
    "    df.loc[df[country] != 0, country] = df.loc[df[country] != 0, country] / df.loc[df[country] != 0, \"country_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a165f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distribute sentiment scores of half and full year calls over the quarters - MIGHT NEED TO ADAPT TO DATE_QUARTER\n",
    "\n",
    "# Initialize a list to store the new rows\n",
    "rows_to_concat = []\n",
    "\n",
    "# Iterate over the rows of the original DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    if row[\"quarter\"] in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
    "        rows_to_concat.append(row.to_frame().T)\n",
    "    elif row[\"quarter\"] == \"HY\":\n",
    "        rows_to_concat.append(row.to_frame().T)\n",
    "        new_row = row.copy()\n",
    "        new_date = row.name - pd.offsets.QuarterEnd(n=1)\n",
    "        new_row.name = new_date\n",
    "        rows_to_concat.append(new_row.to_frame().T)\n",
    "    elif row[\"quarter\"] == \"FY\":\n",
    "        for i in range(4):\n",
    "            new_row = row.copy()\n",
    "            new_date = row.name - pd.offsets.QuarterEnd(n=i)\n",
    "            new_row.name = new_date\n",
    "            rows_to_concat.append(new_row.to_frame().T)\n",
    "\n",
    "# Concatenate the new rows and sort the resulting data frame by the index\n",
    "df = pd.concat(rows_to_concat).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "270ee9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate sentiment scores per country per quarter to obtain sentiment index\n",
    "df_sentiment_index = pd.DataFrame(df[countries_list].groupby(df.index).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89b562f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove years 2002 and 2023, as they are incomplete\n",
    "df_sentiment_index = df_sentiment_index.loc[(df_sentiment_index.index.year != 2002) & (df_sentiment_index.index.year != 2023)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c1f6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column that contains aggregate sentiment of all countries not weighted by country size \n",
    "df_sentiment_index[\"unweighted_aggregate\"] = df_sentiment_index.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abaf0052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 company        date country_count  greece quarter\n",
      "2010-03-31  CTIP3.SA^C17  2010-05-14             1    -3.0      Q1\n",
      "2010-12-31          AB.N  2011-02-10             3    -3.0      Q4\n",
      "2010-03-31   GFNORTEO.MX  2010-04-30             1    -2.0      Q1\n",
      "2010-03-31         MBI.N  2010-05-11             1    -2.0      Q1\n",
      "2010-12-31     TCF.N^H19  2011-01-20             1    -2.0      Q4\n",
      "2010-12-31     FIG.N^L17  2011-03-01             2    -2.0      Q4\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df.nsmallest(100, 'greece')\n",
    "\n",
    "# Select only the desired columns\n",
    "filtered_df = filtered_df[['company', 'date', 'country_count', \"greece\", \"quarter\"]]\n",
    "\n",
    "print(filtered_df[filtered_df.index.year == 2010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ebc0796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            company        date country_count    greece quarter\n",
      "2011-06-30  ERST.VI  2011-07-29           120 -0.100000      Q2\n",
      "2010-03-31  ERST.VI  2010-04-30            55 -0.036364      Q1\n",
      "2014-12-31  ERST.VI  2015-02-27           160 -0.025000      Q4\n",
      "2009-03-31  ERST.VI  2009-04-30            77 -0.012987      Q1\n",
      "2012-09-30  ERST.VI  2012-10-30           111 -0.009009      Q3\n",
      "...             ...         ...           ...       ...     ...\n",
      "2009-06-30  ERST.VI  2009-07-30            66  0.000000      Q2\n",
      "2009-03-31  ERST.VI  2009-04-30            46  0.000000      Q1\n",
      "2011-09-30  ERST.VI  2011-10-28           121  0.000000      Q3\n",
      "2022-12-31  ERST.VI  2022-08-01            60  0.000000      HY\n",
      "2017-06-30  ERST.VI  2017-08-04            98  0.030612      Q2\n",
      "\n",
      "[105 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "erst = df[df[\"company\"] == \"ERST.VI\"]\n",
    "erst = erst[['company', 'date', 'country_count', \"greece\", \"quarter\"]]\n",
    "print(erst.sort_values(\"greece\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd284b27",
   "metadata": {},
   "source": [
    "### First stage: Validation of sentiment index and construction of unwarranted sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c07d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation: sentiment index must be related to fundamentals, r-square important --> create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b436ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If validated: take residuals as unwarranted sentiment measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff9437",
   "metadata": {},
   "source": [
    "### Second stage: Drivers of bond yield spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regress unwarranted sentiment and fundamentals on bond yield spreads, r-square important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44041f6",
   "metadata": {},
   "source": [
    "### Graphs, Figures, and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f1bc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph: debt-to-gdp\n",
    "\n",
    "# Select columns for graph: Germany, Italy, France, Greece, and Spain\n",
    "selected_columns = [\"Germany\", \"Italy\", \"France\", \"Greece\", \"Spain\"]\n",
    "debt_to_gdp_graph = debt_to_gdp[selected_columns]\n",
    "\n",
    "# Plot data as line chart\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Set line colors and styles\n",
    "colors = plt.cm.Reds(np.linspace(0, 1, len(debt_to_gdp_graph.columns)))\n",
    "line_styles = ['-', '--', '-.', ':', '--']\n",
    "\n",
    "# Plot each country's data as a separate line\n",
    "for i, col in enumerate(debt_to_gdp_graph.columns):\n",
    "    ax.plot(debt_to_gdp_graph.index, debt_to_gdp_graph[col], color = colors[i], linestyle = line_styles[i], linewidth = 2, label = col\n",
    "\n",
    "# Set y-axis label\n",
    "ax.set_ylabel(\"Debt-to-GDP Ratio (\\%)\", fontsize = 12)\n",
    "\n",
    "# Set y-axis format as percentage\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: \"{:.0%}\".format(y/100)))\n",
    "\n",
    "# Add legend to plot\n",
    "ax.legend(loc = \"upper center\", bbox_to_anchor = (0.5, -0.1), ncol = 5)\n",
    "\n",
    "# Save plot as PDF file\n",
    "plt.savefig(r\"figures\\debt_to_gdp_plot.pdf\", bbox_inches = \"tight\")\n",
    "\n",
    "# Do not show graph\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2de840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graph: yield spreads\n",
    "\n",
    "# Select columns for graph: Germany, Italy, France, Greece, and Spain\n",
    "selected_columns = [\"italy\", \"france\", \"greece\", \"spain\"]\n",
    "yield_spreads_graph = yield_spreads[selected_columns]\n",
    "\n",
    "# Plot data as line chart\n",
    "fig, ax = plt.subplots(figsize = (8, 6))\n",
    "\n",
    "# Set line colors and styles\n",
    "colors = plt.cm.Reds(np.linspace(0, 1, len(yield_spreads_graph.columns)))\n",
    "line_styles = ['-', '--', '-.', ':', '--']\n",
    "\n",
    "# Plot each country's data as a separate line\n",
    "for i, col in enumerate(yield_spreads_graph.columns):\n",
    "    ax.plot(yield_spreads_graph.index, yield_spreads_graph[col], color = colors[i], linestyle = line_styles[i], linewidth = 2, label = col.capitalize())\n",
    "\n",
    "# Set y-axis label\n",
    "ax.set_ylabel(\"Bond Yield Spread (\\%)\", fontsize = 12)\n",
    "\n",
    "# Set y-axis format as percentage\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: \"{:.0%}\".format(y/100)))\n",
    "\n",
    "# Add legend to plot\n",
    "ax.legend(loc = \"upper center\", bbox_to_anchor=(0.5, -0.1), ncol=5)\n",
    "\n",
    "# Save plot as PDF file\n",
    "plt.savefig(r\"figures\\yield_spreads_plot.pdf\", bbox_inches = \"tight\")\n",
    "\n",
    "# Do not show graph\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "547edff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graphs: sentiment index per country\n",
    "\n",
    "# Plot sentiment as line chart for each EU country\n",
    "for country in countries_list:\n",
    "    \n",
    "    if country in df_sentiment_index.columns:\n",
    "        \n",
    "        # Select columns for graph: Germany, Italy, France, Greece, and Spain\n",
    "        selected_columns = [country]\n",
    "        df_index_graph = df_sentiment_index[selected_columns]\n",
    "\n",
    "        # Plot data as line chart\n",
    "        fig, ax = plt.subplots(figsize = (8, 6))\n",
    "\n",
    "        # Set line colors and styles\n",
    "        colors = plt.cm.Reds(np.linspace(0, 1, len(df_index_graph.columns)) + 1)\n",
    "        line_styles = ['-', '--', '-.', ':', '--']\n",
    "\n",
    "        # Plot each country's data as a separate line; invert score such that negative sentiment is associated with positive y-values \n",
    "        for i, col in enumerate(df_index_graph.columns):\n",
    "            ax.plot(df_index_graph.index, -df_index_graph[col], color = colors[i], linestyle = line_styles[i], linewidth = 1.5, label = col.capitalize())\n",
    "\n",
    "            # Shade negative sentiment red and positive sentiment green\n",
    "            ax.fill_between(df_index_graph.index, -df_index_graph[col], 0, where = (-df_index_graph[col] >= 0), color = \"red\", alpha = 0.1, interpolate = True)\n",
    "            ax.fill_between(df_index_graph.index, -df_index_graph[col], 0, where = (-df_index_graph[col] <= 0), color = \"green\", alpha = 0.1, interpolate = True)\n",
    "\n",
    "        # Add horizontal line at y=0\n",
    "        ax.axhline(0, color = \"grey\", linewidth = .8, zorder = 1)\n",
    "\n",
    "        # Set y-axis label\n",
    "        ax.set_ylabel(\"{} Sentiment Score (inverted)\".format(country.title()), fontsize = 12)\n",
    "        \n",
    "        # Remove top and right lines of box\n",
    "        # ax.spines[\"top\"].set_visible(False)\n",
    "        # ax.spines[\"right\"].set_visible(False)\n",
    "        \n",
    "        # Save plot as PDF file and close plot\n",
    "        plt.savefig(r\"figures\\sentiment_index\\{}_sentiment_plot.pdf\".format(country), bbox_inches = \"tight\")\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b7ab0",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2be4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO\n",
    "\n",
    "# Also get interest rates + QE from Eikon?\n",
    "# Get remaining transcripts from eikon\n",
    "# Delete questions from analysts?\n",
    "# +/- 10 words algorithm cannot overlap answers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
